## AI-generated Dandiset Summaries

These Dandiset summaries were generated by an LLM using prompts created by the provided script. Each prompt is based on the content of the NWB files in the Dandiset and metadata for the Dandiset.

See the [dandisets directory](./dandisets) for the summaries generated thus far.

To generate a prompt for creating a summary:

```bash
cd dandiset_summarizer

# Replace 000953 with the Dandiset ID you want to summarize
python dandiset_summarizer/dandiset_summarizer.py create-dandiset-summary --dandiset-id 000953 --dandiset-version draft --cache-dir cache
```

This will create the summary in the `dandisets` directory, if it doesn't already exist.

To generate summaries for all public Dandisets:

```bash
python dandiset_summarizer/dandiset_summarizer.py create-dandiset-summaries
```

To create embeddings for the summaries:

```bash
python dandiset_summarizer/dandiset_summarizer.py create-embeddings
```

## Semantic embeddings

These summaries were then used to generate semantic embeddings, one vector per Dandiset, which is used to power the [semantic search on Neurosift](https://neurosift.app/?p=/dandi-query) (click "Search by abstract").

## How it works

First, NWB files are clustered together based on the structure of their content. This dramatically reduces the number of unique NWB files that need to be summarized since Dandisets usually contain many similar files. The script then creates a text summary of representative files from each cluster. This summary includes meta data such as the experiment description as well as the paths of neurodata object, their neurodata types, descriptions and the shape and datatypes of the datasets. These are then glued together to form a single prompt that can be used to generate a summary using a chatbot.

The script caches the LINDI files in a local directory to avoid repeated downloads. It also caches the list of NWB files in the Dandiset to avoid repeated calls to the DANDI API.
